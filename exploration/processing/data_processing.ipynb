{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import re\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import warnings\n",
    "import pdfplumber\n",
    "from spellchecker import SpellChecker\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data/Study permit_ Who can apply - Canada.ca.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_headers_and_footers(pdf_path):\n",
    "    header = \"\"\n",
    "    footers = []\n",
    "    ref_link = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            txt = page.extract_text_lines()\n",
    "            \n",
    "            if not header:\n",
    "                header = txt[0]['text']\n",
    "                \n",
    "            footer = txt[-1]['text']\n",
    "            if footer not in footers:\n",
    "                footers.append(footer)\n",
    "                \n",
    "            ref_link = footer.split()[0]\n",
    "                \n",
    "    return header, footers, ref_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, footers, ref_link = detect_headers_and_footers(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content, txt_removed=None):\n",
    "    content = content.encode('utf-8').decode('utf-8')\n",
    "    content = re.sub(r'\\ue107', '', content)\n",
    "    content = re.sub(r'\\n', ' ', content)\n",
    "    content = content.replace(f'{header}', '')\n",
    "    if txt_removed is not None:\n",
    "        for txt in txt_removed:\n",
    "            content = content.replace(txt, '')\n",
    "    content = re.sub(r'\\ue092 Date modified: \\d\\d\\d\\d-\\d\\d-\\d\\d', '', content)\n",
    "    for footer in footers:\n",
    "        content = content.replace(footer, '')\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperlinks(pdf_path):\n",
    "    hyperlinks = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for _, page in enumerate(pdf.pages):\n",
    "            for annotation in page.annots:\n",
    "                if str(annotation['uri']).startswith(r'http://') or str(annotation['uri']).startswith(r'https://'):\n",
    "                    uri = annotation.get(\"uri\", None)\n",
    "                    if uri:\n",
    "                        # Get the bounding box coordinates for the link\n",
    "                        x0, y0, x1, y1 = annotation['x0'], annotation['top'], annotation['x1'], annotation['bottom']\n",
    "                        \n",
    "                        # Extract text within the bounding box\n",
    "                        text_content = \"\"\n",
    "                        for char in page.chars:\n",
    "                            if x0 <= char['x0'] and char['x1'] <= x1 and y0 <= char['top'] and char['bottom'] <= y1:\n",
    "                                text_content += char.get('text', '')\n",
    "                        \n",
    "                        hyperlink = {\n",
    "                            \"uri\": uri,\n",
    "                            \"text\": text_content.strip()\n",
    "                        }\n",
    "                        hyperlinks.append(hyperlink)\n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tables(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Extract tables for each page\n",
    "        tables = [pdf.pages[i].find_tables() for i in range(len(pdf.pages))]\n",
    "\n",
    "        # Check if any table has more than 2 rows and 2 columns\n",
    "        tables_found = False\n",
    "        for page_tables in tables:\n",
    "            for table in page_tables:\n",
    "                if len(table.rows) > 2 and len(table.rows[0]) > 2:\n",
    "                    tables_found = True\n",
    "                    break\n",
    "            if tables_found:\n",
    "                break\n",
    "\n",
    "        return tables_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_section_with_content(pdf_path, skip_tags=None, category=None):\n",
    "    if skip_tags is None:\n",
    "        skip_tags = []\n",
    "    \n",
    "    sections_with_content = []\n",
    "    current_section = None\n",
    "    current_subsection = None\n",
    "    current_content = []\n",
    "    subsection_indexes = {}\n",
    "    \n",
    "    \n",
    "    SECTION_MIN_SIZE = 28\n",
    "    SECTION_MAX_SIZE = 29\n",
    "    SUBSECTION_MIN_SIZE = 26\n",
    "    SUBSECTION_MAX_SIZE = 27\n",
    "    is_section = False\n",
    "    is_subsection = False\n",
    "    \n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        if not check_tables(pdf_path):\n",
    "            is_section = [(line['chars'][0]['fontname'] == 'CAAAAA+Lato-Bold' and SECTION_MIN_SIZE <= line['chars'][0]['size'] < SECTION_MAX_SIZE) for page in pdf.pages for line in page.extract_text_lines()]\n",
    "            is_subsection = [(line['chars'][0]['fontname'] == 'CAAAAA+Lato-Bold' and SUBSECTION_MIN_SIZE <= line['chars'][0]['size'] < SUBSECTION_MAX_SIZE) for page in pdf.pages for line in page.extract_text_lines()]\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                text_with_coords = page.extract_text_lines()\n",
    "                for line in text_with_coords:\n",
    "                    text = line['text']\n",
    "                    \n",
    "                    if line['chars'][0]['fontname'] == 'CAAAAA+Lato-Bold' and SECTION_MIN_SIZE <= line['chars'][0]['size'] < SECTION_MAX_SIZE:\n",
    "                        if current_section and current_content:\n",
    "                            sections_with_content[-1]['content'] = ' '.join(current_content).strip()\n",
    "                        if text not in skip_tags:  # Skip section if in skip_tags\n",
    "                            current_section = line['text']\n",
    "                            current_content = []\n",
    "                            if category != None:\n",
    "                                sections_with_content.append({\n",
    "                                    'tag': category,\n",
    "                                    'section': current_section,\n",
    "                                    'subsections': [],\n",
    "                                    'content': ''\n",
    "                                })\n",
    "                            else:\n",
    "                                sections_with_content.append({\n",
    "                                    'section': current_section,\n",
    "                                    'subsections': [],\n",
    "                                    'content': ''\n",
    "                                })\n",
    "                            section_index = len(sections_with_content) - 1\n",
    "                            subsection_indexes = {}\n",
    "                    elif line['chars'][0]['fontname'] == 'CAAAAA+Lato-Bold' and SUBSECTION_MIN_SIZE <= line['chars'][0]['size'] < SUBSECTION_MAX_SIZE:\n",
    "                        if current_subsection and current_content:\n",
    "                            subsection_content = current_subsection + \": \" + clean_content(' '.join(current_content).strip())\n",
    "                            if current_subsection in subsection_indexes:\n",
    "                                sections_with_content[section_index]['subsections'][subsection_indexes[current_subsection]]['content'] = subsection_content\n",
    "                            else:\n",
    "                                if current_subsection not in skip_tags:  # Skip subsection if in skip_tags\n",
    "                                    sections_with_content[section_index]['subsections'].append({\n",
    "                                        'content': subsection_content\n",
    "                                    })\n",
    "                                    subsection_indexes[current_subsection] = len(sections_with_content[section_index]['subsections']) - 1\n",
    "                        current_subsection = line['text']\n",
    "                        current_content = []\n",
    "                    elif current_subsection != \"On this page\":\n",
    "                        current_content.append(line['text'])\n",
    "                    \n",
    "                        \n",
    "                    if current_subsection and current_content:\n",
    "                        subsection_content = current_subsection + \": \" + clean_content(' '.join(current_content).strip())\n",
    "                        if current_subsection in subsection_indexes:\n",
    "                            sections_with_content[section_index]['subsections'][subsection_indexes[current_subsection]]['content'] = subsection_content\n",
    "                        else:\n",
    "                            if current_subsection not in skip_tags:  # Skip subsection if in skip_tags\n",
    "                                sections_with_content[section_index]['subsections'].append({\n",
    "                                    'content': subsection_content\n",
    "                                })\n",
    "                                subsection_indexes[current_subsection] = len(sections_with_content[section_index]['subsections']) - 1   \n",
    "                    elif current_section and current_content:\n",
    "                        sections_with_content[-1]['content'] = clean_content(' '.join(current_content).strip())\n",
    "                        \n",
    "                    \n",
    "                    # handle no section or subsection detected\n",
    "                    if not any(is_section) and not any(is_subsection) and current_content:\n",
    "                        current_section = \"Other Resources\"\n",
    "                        sections_with_content.append({\n",
    "                            'section': current_section,\n",
    "                            'subsections': [],\n",
    "                            'content': clean_content(' '.join(current_content).strip())\n",
    "                        })\n",
    "            return sections_with_content\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "            # 1. Extract text excluding tables\n",
    "            # 2. Extract text within bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = detect_section_with_content(pdf_path, skip_tags=[\"On this page\", \"Study permit\"], category=\"Study Permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsection_content(sections):\n",
    "    for section in sections:\n",
    "        subsection_content = ' '.join([subsection['content'] for subsection in section['subsections']])\n",
    "        section['embedding_text'] = section['section'] + \" > \" + section['content'] + \" \" + subsection_content\n",
    "        \n",
    "        # Remove content and subsections\n",
    "        del section['content']\n",
    "        del section['subsections']\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hyperlinks(hyperlinks, doc):\n",
    "    content = doc['embedding_text']\n",
    "    filtered_hyperlinks = []\n",
    "    for hyperlink in hyperlinks:\n",
    "        if hyperlink['text'].lower() in content.lower() and hyperlink['text'] != '':\n",
    "            filtered_hyperlinks.append(hyperlink)\n",
    "    return filtered_hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_document(hyperlinks, docs):\n",
    "    for doc in docs:\n",
    "        doc['hyperlinks'] = filter_hyperlinks(hyperlinks, doc)\n",
    "    return docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spacy_env)",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
